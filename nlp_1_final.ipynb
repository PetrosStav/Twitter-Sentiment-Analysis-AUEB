{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp-1-final.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B72CloGwDLk",
        "colab_type": "text"
      },
      "source": [
        "# Natural Language Processing - Exercise 1 (Problem 4)\n",
        "\n",
        "## Authors\n",
        "* Charteros Eleftherios ([l.harteros@gmail.com](mailto:l.harteros@gmail.com))\n",
        "* Kotitsas Sotirios ([sotiriskot9@gmail.com](mailto:sotiriskot9@gmail.com))\n",
        "* Stavropoulos Petros ([pstav1993@gmail.com](mailto:pstav1993@gmail.com))\n",
        "* Xenouleas Efstratios ([stratosxen@gmail.com](mailto:stratosxen@gmail.com))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGDo9i5mT4mj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import everything\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "from nltk.corpus import gutenberg, brown, stopwords\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams, pad_sequence\n",
        "from pprint import pprint\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHaD_U8pUJwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "outputId": "f7fc1df4-a0cb-43a2-fe83-fcd303042a8e"
      },
      "source": [
        "# Set seed for random\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "# ALPHA is for a-smoothing\n",
        "# N is the minimum frequency for a word to include it in the vocab\n",
        "ALPHA = 0.01\n",
        "N = 10\n",
        "\n",
        "# Download corpuses, punctuation and stopwords\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Print the nltk version\n",
        "print(nltk.__version__)\n",
        "print(gutenberg.fileids())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "3.2.5\n",
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU8xpTHqVJ-5",
        "colab_type": "text"
      },
      "source": [
        "# Part A"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4weBG_y6UR0M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get a specific text from gutenberg corpus\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "\n",
        "# Or get all the gutenberg texts (for testing)\n",
        "# text = ''\n",
        "# for fid in gutenberg.fileids():\n",
        "#   text += gutenberg.raw(fid) + ' '\n",
        "# text = text[:-1]\n",
        "\n",
        "# Remove some characters from text\n",
        "text = text.translate(str.maketrans('', '', \"()[]/:,;-_\\\"'*\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBw3lEtYUS0V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6be47080-81a6-49af-9efa-480c11e3f1ce"
      },
      "source": [
        "# Perform sentence splitting/tokenization to the text\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "# Perform word tokenization for each sentence and add the tokenized words to the list\n",
        "sentences_tokenized = []\n",
        "for sent in tqdm(sentences):\n",
        "    tokens = word_tokenize(sent.lower())\n",
        "    sentences_tokenized.append(tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 7738/7738 [00:01<00:00, 5005.72it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGc09I7gUV4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split train - val - test\n",
        "# Use 80% of the sentences as a training set\n",
        "\n",
        "train_size = int(len(sentences_tokenized) * 0.8)\n",
        "test_size = train_size + int((len(sentences_tokenized) - train_size) / 2)\n",
        "\n",
        "train_set = sentences_tokenized[:train_size]\n",
        "val_set = sentences_tokenized[train_size:test_size]\n",
        "test_set = sentences_tokenized[test_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLsoPS7BUZZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Flatten the sentences of the training set into words\n",
        "words = [item for sublist in train_set for item in sublist]\n",
        "\n",
        "# Find the frequency of words in the train set\n",
        "freq = Counter()\n",
        "freq.update(words)\n",
        "\n",
        "# Create the vocab from the train set for words with frequency more than the specified\n",
        "vocab = set()\n",
        "for sent in train_set:\n",
        "    vocab.update([word for word in sent if freq[word] >= N])\n",
        "vocab.add('<UNK>')\n",
        "vocab.add('<start>')\n",
        "vocab.add('<start1>')\n",
        "vocab.add('<start2>')\n",
        "vocab.add('<end>')\n",
        "\n",
        "# Create a dictionary from the set to use as a vocab\n",
        "vocab = {word: i for i, word in enumerate(vocab)}\n",
        "inv_vocab = {vocab[word]: word for word in vocab}\n",
        "\n",
        "vocab_size = len(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvmhiR1BUcUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replace unkown words in the sets\n",
        "# (we could also use the defaultdict nltk module)\n",
        "for i in range(len(train_set)):\n",
        "    for j in range(len(train_set[i])):\n",
        "        if train_set[i][j] not in vocab:\n",
        "            train_set[i][j] = '<UNK>'\n",
        "for i in range(len(val_set)):\n",
        "    for j in range(len(val_set[i])):\n",
        "        if val_set[i][j] not in vocab:\n",
        "            val_set[i][j] = '<UNK>'\n",
        "for i in range(len(test_set)):\n",
        "    for j in range(len(test_set[i])):\n",
        "        if test_set[i][j] not in vocab:\n",
        "            test_set[i][j] = '<UNK>'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mdRi2s9Uigt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Count ngrams\n",
        "unigram_counter = Counter()\n",
        "bigram_counter = Counter()\n",
        "trigram_counter = Counter()\n",
        "\n",
        "# For each sentence in the training set get the ngrams and update the according Counter\n",
        "for sent in train_set:\n",
        "    unigram_counter.update(sent)\n",
        "    bigrams = ngrams(['<start>'] + sent + ['<end>'], 2)\n",
        "    bigram_counter.update(bigrams)\n",
        "    trigrams = ngrams(['<start1>'] + ['<start2>'] + sent + ['<end>'], 3)\n",
        "    trigram_counter.update(trigrams)\n",
        "\n",
        "# Add the special start tokens to the counters, in order for them to be used by the next-order ngram models\n",
        "unigram_counter['<start>'] = len(train_set)\n",
        "bigram_counter[('<start1>', '<start2>')] = len(train_set)\n",
        "\n",
        "# Copy all the bigrams that start with the <start> token to bigrams that start with <start2> tokens in order\n",
        "# to avoide zero probability on trigrams that start with the <start2> token\n",
        "# ex. Calculating P(product| <start2>, the) = C(<start2>, the, product) / C(<start2>, the)\n",
        "to_add = list()\n",
        "for k in bigram_counter.keys():\n",
        "    if k[0] == '<start>':\n",
        "        to_add.append(('<start2>', k[1],  bigram_counter[k]))\n",
        "for k in to_add:\n",
        "    bigram_counter[(k[0], k[1])] = k[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lCPpVf8VUgd",
        "colab_type": "text"
      },
      "source": [
        "# Part B"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggic7UfjUn6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the log probability of a sentence to appear on a language model using different ngram models\n",
        "def predict(sent, ngram):\n",
        "    if ngram not in [1, 2, 3]:\n",
        "        print('Please choose either a unigram, bigram or trigram language model')\n",
        "        return None\n",
        "\n",
        "    # Find the aggregated log probability of the sentence by adding the log probabilities\n",
        "    # of each ngram in the sentence\n",
        "    prob = 0\n",
        "    if ngram == 1:\n",
        "        # Sum of all unigrams (count of all words in the train set)\n",
        "        C = sum(unigram_counter.values())\n",
        "        for i in range(0, len(sent) - (ngram - 1)):\n",
        "            unigram_prob = (unigram_counter[sent[i]] + ALPHA) / (C + ALPHA * vocab_size)\n",
        "            prob += math.log2(unigram_prob)\n",
        "    elif ngram == 2:\n",
        "        # Pad sentence depending on the ngram parameter\n",
        "        sent = ['<start>'] + sent + ['<end>']\n",
        "        for i in range(0, len(sent) - (ngram - 1)):\n",
        "            bigram_prob = (bigram_counter[(sent[i], sent[i + 1])] + ALPHA) / (\n",
        "                        unigram_counter[sent[i]] + ALPHA * vocab_size)\n",
        "            prob += math.log2(bigram_prob)\n",
        "    else:\n",
        "        # Pad sentence depending on the ngram parameter\n",
        "        sent = ['<start1>'] + ['<start2>'] + sent + ['<end>']\n",
        "        for i in range(0, len(sent) - (ngram - 1)):\n",
        "            trigram_prob = (trigram_counter[(sent[i], sent[i + 1], sent[i + 2])] + ALPHA) / (\n",
        "                        bigram_counter[(sent[i], sent[i + 1])] + ALPHA * vocab_size)\n",
        "            prob += math.log2(trigram_prob)\n",
        "\n",
        "    return prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT_xCB-TUo8Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "4d0ee13a-e7b4-474a-ff0e-7da8ea42be2b"
      },
      "source": [
        "# In this part we will compare the log probabilities of some of the sentences in the test dataset\n",
        "# with the one's in random words from the vocab.\n",
        "# We presume that the log probability of the sentences from the test dataset will be much higher\n",
        "# than the random vocab words (of the same length)\n",
        "\n",
        "# Print the probabilities of 5 sentences from the test set\n",
        "\n",
        "print()\n",
        "print('Evalutate Real-Fake Sentences Log Probabilities')\n",
        "for i, sent in enumerate(test_set):\n",
        "    print()\n",
        "    print('Real Sentence {}'.format(i+1))\n",
        "    print(' '.join(sent))\n",
        "    # Create a random sentence from the vocab of the same length\n",
        "    rand_sent = [inv_vocab[random.randint(0, vocab_size-1)] for _ in range(len(sent))]\n",
        "    print()\n",
        "    print('Fake sentence {}'.format(i+1))\n",
        "    print(' '.join(rand_sent))\n",
        "    # Using the unigram language model\n",
        "    prob = predict(sent, 1)\n",
        "    prob2 = predict(rand_sent, 1)\n",
        "    print()\n",
        "    print('Unigram model')\n",
        "    print('Real: {} -- Fake: {}'.format(prob, prob2))\n",
        "    # Using the bigram language model\n",
        "    prob = predict(sent, 2)\n",
        "    prob2 = predict(rand_sent, 2)\n",
        "    print()\n",
        "    print('Bigram model')\n",
        "    print('Real: {} -- Fake: {}'.format(prob, prob2))\n",
        "    # Using the trigram language model\n",
        "    prob = predict(sent, 3)\n",
        "    prob2 = predict(rand_sent, 3)\n",
        "    print()\n",
        "    print('Trigram model')\n",
        "    print('Real: {} -- Fake: {}'.format(prob, prob2))\n",
        "    if i == 2: break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Evalutate Real-Fake Sentences Log Probabilities\n",
            "\n",
            "Real Sentence 1\n",
            "he would <UNK> himself from <UNK> again such <UNK> <UNK> <UNK> had gone to <UNK> to be indifferent .\n",
            "\n",
            "Fake sentence 1\n",
            "laughing expect beautiful standing world regret since small affection am from means greatest turning my robert hurry ought disposition\n",
            "\n",
            "Unigram model\n",
            "Real: -123.32987405202242 -- Fake: -223.78882939457165\n",
            "\n",
            "Bigram model\n",
            "Real: -119.77226892195883 -- Fake: -255.16206716897685\n",
            "\n",
            "Trigram model\n",
            "Real: -155.29472709170244 -- Fake: -212.15972069905936\n",
            "\n",
            "Real Sentence 2\n",
            "but he had gone to a wrong place .\n",
            "\n",
            "Fake sentence 2\n",
            "loved confidence society hearing miss oh churchill intimacy however\n",
            "\n",
            "Unigram model\n",
            "Real: -70.83922991680878 -- Fake: -102.50672274575095\n",
            "\n",
            "Bigram model\n",
            "Real: -54.96483148523077 -- Fake: -127.37364788912711\n",
            "\n",
            "Trigram model\n",
            "Real: -68.9576437358984 -- Fake: -110.7456088224108\n",
            "\n",
            "Real Sentence 3\n",
            "there was too much <UNK> happiness in his <UNK> house woman <UNK> too amiable a form in it isabella was too much like <UNK> only in those <UNK> <UNK> which always brought the other in <UNK> before him for much to have been done even had his time been <UNK> had <UNK> on however <UNK> day after <UNK> this very <UNK> <UNK> had <UNK> the history of jane <UNK> with the <UNK> which must be felt <UNK> which he did not <UNK> to feel having never believed frank churchill to be at all <UNK> emma was there so much fond <UNK> so much <UNK> anxiety for her that he could stay no longer .\n",
            "\n",
            "Fake sentence 3\n",
            "he place minutes guess ten certainly absence trying day either fathers smallest thinking heard invited wife speech caught pleasure seldom however rational share occupied door felt himself nature beyond altogether sent happened situation compliments isabella cheerful impatient how awkward tolerably opened view occasion nature me road uncle them desired usual for difference encouragement reason asked servants martins distress opinion to besides obliged judge westons hand wait opinion wanting need companions hers will stand randalls left plain fine did off because come feel was situation possible long back already influence listen ones thrown supposed first assure think civil beautiful nor returned sweet look evening course marriage welcome things leave behaviour tender take dreadful curiosity sit\n",
            "\n",
            "Unigram model\n",
            "Real: -851.5644092180667 -- Fake: -1349.8448358344694\n",
            "\n",
            "Bigram model\n",
            "Real: -786.9999328789028 -- Fake: -1394.6114691575485\n",
            "\n",
            "Trigram model\n",
            "Real: -936.6490196907052 -- Fake: -1167.1187278318096\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24M5ZqTQVcFx",
        "colab_type": "text"
      },
      "source": [
        "# Part C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjAoJ-xiVeKL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method that calculates the cross entropy of a list of sentences\n",
        "# implemented according to https://en.wikipedia.org/wiki/Cross_entropy\n",
        "def cross_entropy(sentences, ngram):\n",
        "    # We must treat the list of sentenes as a big sentence according to the exercise so\n",
        "    # loop through all the sentences and aggregate the probabilities and the sentences\n",
        "    prob = 0\n",
        "    size = 0\n",
        "    for sent in sentences:\n",
        "        prob += predict(sent, ngram)\n",
        "        size += len(sent) + 1 if ngram != 1 else len(sent)\n",
        "\n",
        "    # Divide by the size to get the cross entropy estimation\n",
        "    return -(prob / size)\n",
        "\n",
        "# Method that calculates the perplexity for a list of sentences using the language models\n",
        "def perplexity(sentences, ngram):\n",
        "    perplexity = 2 ** cross_entropy(sentences, ngram)\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vuxg5piVqB4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "5f2f91d4-3507-4727-a637-9c2d09ad4226"
      },
      "source": [
        "print(\"2-gram model language cross-entropy : \", cross_entropy(test_set, 2))\n",
        "print(\"2-gram model language perplexity : \", perplexity(test_set, 2))\n",
        "print()\n",
        "print(\"3-gram model language cross-entropy : \", cross_entropy(test_set, 3))\n",
        "print(\"3-gram model language perplexity : \", perplexity(test_set, 3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2-gram model language cross-entropy :  6.259240843606695\n",
            "2-gram model language perplexity :  76.59831990811904\n",
            "\n",
            "3-gram model language cross-entropy :  7.562849465119887\n",
            "3-gram model language perplexity :  189.07954201515824\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K16T4meoVzD3",
        "colab_type": "text"
      },
      "source": [
        "# Part D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8nC-iNtV0tn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use linear interpolation to combine the predictions of the 2 language models\n",
        "def linear_predict(sent, lam=[0.5 , 0.5]):\n",
        "    # Pad sentence\n",
        "    sentence = ['<start1>'] + ['<start2>'] + sent + ['<end>']\n",
        "\n",
        "    # Find the aggragated log probability of the sentence using linear interpolation of the\n",
        "    # trigram and bigram probabilities for each word\n",
        "    prob = 0\n",
        "    for i in range(0, len(sentence) - 2):\n",
        "        trigram_prob = (trigram_counter[(sentence[i], sentence[i + 1], sentence[i + 2])] + ALPHA) / (\n",
        "                    bigram_counter[(sentence[i], sentence[i + 1])] + ALPHA * vocab_size)\n",
        "    \n",
        "        bigram_prob = (bigram_counter[(sentence[i+1], sentence[i + 2])] + ALPHA) / (\n",
        "                    unigram_counter[sentence[i+1]] + ALPHA * vocab_size)\n",
        "\n",
        "        # Linear interpolation of probabilities\n",
        "        prob += math.log2(lam[0] * bigram_prob + lam[1] * trigram_prob)\n",
        "\n",
        "    return prob\n",
        "    \n",
        "# Cross entropy using linear interpolation of language models\n",
        "def linear_cross_entropy(sentences, lam=[0.5, 0.5]):\n",
        "    # We must treat the list of sentenes as a big sentence according to the exercise so\n",
        "    # loop through all the sentences and aggregate the probabilities and the sentences\n",
        "    prob = 0\n",
        "    size = 0\n",
        "    for sent in sentences:\n",
        "        prob += linear_predict(sent, lam)\n",
        "        size += len(sent) + 1\n",
        "    # Divide by the size to get the cross entropy estimation\n",
        "    return -(prob / size)\n",
        "\n",
        "# Perplexity using linear interpolation of language models\n",
        "def linear_perplexity(sentences, lam=[0.5, 0.5]):\n",
        "    perplexity = 2 ** linear_cross_entropy(sentences, lam)\n",
        "    return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPYbiH29WDZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "23a0fe40-bf4f-4b18-abb8-dd51a1d7fda0"
      },
      "source": [
        "print(\"2-gram model language cross-entropy : \", cross_entropy(test_set, 2))\n",
        "print(\"2-gram model language perplexity : \", perplexity(test_set, 2))\n",
        "print()\n",
        "print(\"3-gram model language cross-entropy : \", cross_entropy(test_set, 3))\n",
        "print(\"3-gram model language perplexity : \", perplexity(test_set, 3))\n",
        "print()\n",
        "print(\"linear model language cross-entropy : \", linear_cross_entropy(test_set))\n",
        "print(\"linear model language perplexity : \", linear_perplexity(test_set))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2-gram model language cross-entropy :  6.259240843606695\n",
            "2-gram model language perplexity :  76.59831990811904\n",
            "\n",
            "3-gram model language cross-entropy :  7.562849465119887\n",
            "3-gram model language perplexity :  189.07954201515824\n",
            "\n",
            "linear model language cross-entropy :  5.839592381453616\n",
            "linear model language perplexity :  57.265422501252935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2Spt1MqWIzh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method that fine-tunes the lambda parameters for the linear interpolation for a given set of sentences\n",
        "def train_linear(sentences, step):\n",
        "    best = float('inf')\n",
        "    best_lam = [0.5, 0.5]\n",
        "    vals = np.arange(0, 1, step)\n",
        "    for v in tqdm(vals):\n",
        "        lamdas = [v, 1-v]\n",
        "        cross = linear_cross_entropy(sentences, lam=lamdas)\n",
        "        if cross < best:\n",
        "            best = cross\n",
        "            best_lam = lamdas\n",
        "    return best_lam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzFcwhTBWTUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "outputId": "09301942-6cad-4d40-b3a6-b233ecb64670"
      },
      "source": [
        "# Find the best lambdas\n",
        "lamdas = train_linear(val_set, 0.001)\n",
        "\n",
        "print()\n",
        "print(\"Best lamdas : \", lamdas)\n",
        "\n",
        "# Get the log probability on test set using the default lambdas and then the tuned lambdas\n",
        "for i, sent in enumerate(test_set):\n",
        "    print()\n",
        "    print('Sentence {}'.format(i+1))\n",
        "    print(' '.join(sent))\n",
        "    print()\n",
        "    print('Default lambdas log probability')\n",
        "    prob = linear_predict(sent)\n",
        "    print(prob)\n",
        "    print(\"----------\")\n",
        "    print('Tuned lambdas log probability')\n",
        "    prob = linear_predict(sent, lam=lamdas)\n",
        "    print(prob)\n",
        "    print(\"----------\")\n",
        "    if i == 2: break\n",
        "\n",
        "print()\n",
        "print(\"default linear model language cross-etropy : \", linear_cross_entropy(test_set))\n",
        "print(\"default linear model language perplexity : \", linear_perplexity(test_set))\n",
        "print()\n",
        "print(\"tuned linear model language cross-etropy : \", linear_cross_entropy(test_set, lam=lamdas))\n",
        "print(\"tuned linear model language perplexity : \", linear_perplexity(test_set, lam=lamdas))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:54<00:00, 17.76it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Best lamdas :  [0.774, 0.22599999999999998]\n",
            "\n",
            "Sentence 1\n",
            "he would <UNK> himself from <UNK> again such <UNK> <UNK> <UNK> had gone to <UNK> to be indifferent .\n",
            "\n",
            "Default lambdas log probability\n",
            "-116.48621045502652\n",
            "----------\n",
            "Tuned lambdas log probability\n",
            "-112.29302895024749\n",
            "----------\n",
            "\n",
            "Sentence 2\n",
            "but he had gone to a wrong place .\n",
            "\n",
            "Default lambdas log probability\n",
            "-48.734729206985946\n",
            "----------\n",
            "Tuned lambdas log probability\n",
            "-46.64366928326629\n",
            "----------\n",
            "\n",
            "Sentence 3\n",
            "there was too much <UNK> happiness in his <UNK> house woman <UNK> too amiable a form in it isabella was too much like <UNK> only in those <UNK> <UNK> which always brought the other in <UNK> before him for much to have been done even had his time been <UNK> had <UNK> on however <UNK> day after <UNK> this very <UNK> <UNK> had <UNK> the history of jane <UNK> with the <UNK> which must be felt <UNK> which he did not <UNK> to feel having never believed frank churchill to be at all <UNK> emma was there so much fond <UNK> so much <UNK> anxiety for her that he could stay no longer .\n",
            "\n",
            "Default lambdas log probability\n",
            "-766.9371003195782\n",
            "----------\n",
            "Tuned lambdas log probability\n",
            "-757.513776478326\n",
            "----------\n",
            "\n",
            "default linear model language cross-etropy :  5.839592381453616\n",
            "default linear model language perplexity :  57.265422501252935\n",
            "\n",
            "tuned linear model language cross-etropy :  5.747359026897324\n",
            "tuned linear model language perplexity :  53.718943526255394\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}